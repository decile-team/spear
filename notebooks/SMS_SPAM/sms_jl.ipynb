{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***End to End tutorial for SMS_SPAM labeling using JL:***\n",
    "**The paper and documentation can be found here:** [Paper](https://arxiv.org/abs/2008.09887), [Documentation](https://spear-decile.readthedocs.io/en/latest/#joint-learning-jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "User don't need to include this cell to use the package\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../../') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Defining an Enum to hold labels:***\n",
    "### **Representation of class Labels**\n",
    "\n",
    "<p>All the class labels for which we define labeling functions are encoded in enum and utilized in our next tasks. Make sure not to define an Abstain(Labeling function(LF) not deciding anything) class inside this Enum, instead import the ABSTAIN object as used later in LF section.</p>\n",
    "\n",
    "<p>SPAM dataset contains 2 classes i.e <b>HAM</b> and <b>SPAM</b>. Note that the numbers we associate can be anything but it is suggested to use a continuous numbers from 0 to number_of_classes-1</p>\n",
    "\n",
    "<p><b>**Note that even though this example is a binary classification, this(SPEAR) library supports multi-class classification**</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# enum to hold the class labels\n",
    "class ClassLabels(enum.Enum):\n",
    "    SPAM = 1\n",
    "    HAM = 0\n",
    "\n",
    "THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Defining preprocessors, continuous_scorers, labeling functions:***\n",
    "During labeling the unlabelled data we lookup for few keywords to assign a class SMS.\n",
    "\n",
    "<b>Example</b> : *If a message contains apply or buy in it then most probably the message is spam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigWord1 = {\"free\",\"credit\",\"cheap\",\"apply\",\"buy\",\"attention\",\"shop\",\"sex\",\"soon\",\"now\",\"spam\"}\n",
    "trigWord2 = {\"gift\",\"click\",\"new\",\"online\",\"discount\",\"earn\",\"miss\",\"hesitate\",\"exclusive\",\"urgent\"}\n",
    "trigWord3 = {\"cash\",\"refund\",\"insurance\",\"money\",\"guaranteed\",\"save\",\"win\",\"teen\",\"weight\",\"hair\"}\n",
    "notFreeWords = {\"toll\",\"Toll\",\"freely\",\"call\",\"meet\",\"talk\",\"feedback\"}\n",
    "notFreeSubstring = {\"not free\",\"you are\",\"when\",\"wen\"}\n",
    "firstAndSecondPersonWords = {\"I\",\"i\",\"u\",\"you\",\"ur\",\"your\",\"our\",\"we\",\"us\",\"youre\"}\n",
    "thirdPersonWords = {\"He\",\"he\",\"She\",\"she\",\"they\",\"They\",\"Them\",\"them\",\"their\",\"Their\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Declaration of a simple preprocessor function**\n",
    "\n",
    "\n",
    "For most of the tasks in NLP, computer vivsion instead of using the raw datapoint we preprocess the datapoint and then label it. Preprocessor functions are used to preprocess an instance before labeling it. We use **`@preprocessor(name,resources)`** decorator to declare a function as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import preprocessor\n",
    "\n",
    "\n",
    "@preprocessor(name = \"LOWER_CASE\")\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "lower = convert_to_lower(\"RED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some Labeling function(LF) definitions**\n",
    "Below are some examples on how to define LFs and continuous LFs(CLFs). To get the continuous score for a CLF, we need to define a function with continuous_scorer decorator(just like labeling_function decorator) and pass it to a CLF as displayed below. Also note how the continuous score can be used in CLF. Note that the word_similarity is the function with continuous_scorer decorator and is written in con_scorer file(this file is not a part of package) in same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import labeling_function, ABSTAIN\n",
    "\n",
    "from helper.con_scorer import word_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "@preprocessor()\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF1(c,**kwargs):    \n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF2(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF3(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM \n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF4(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF5(c,**kwargs):\n",
    "    for pattern in kwargs[\"keywords\"]:    \n",
    "        if \"free\" in c.split() and re.search(pattern,c, flags= re.I):\n",
    "            return ClassLabels.HAM\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF6(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF7(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(label=ClassLabels.SPAM)\n",
    "def LF8(c,**kwargs):\n",
    "    if (sum(1 for ch in c if ch.isupper()) > 6):\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF1(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF2(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF3(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF4(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF5(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF6(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF7(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=lambda x: 1-np.exp(float(-(sum(1 for ch in x if ch.isupper()))/2)),label=ClassLabels.SPAM)\n",
    "def CLF8(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Accumulating all LFs into rules, an LFset(a class) object:***\n",
    "### **Importing LFSet and passing LFs we defined, to that class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import LFSet\n",
    "\n",
    "LFS = [LF1,\n",
    "    LF2,\n",
    "    LF3,\n",
    "    LF4,\n",
    "    LF5,\n",
    "    LF6,\n",
    "    LF7,\n",
    "    LF8,\n",
    "    CLF1,\n",
    "    CLF2,\n",
    "    CLF3,\n",
    "    CLF4,\n",
    "    CLF5,\n",
    "    CLF6,\n",
    "    CLF7,\n",
    "    CLF8\n",
    "      ]\n",
    "\n",
    "rules = LFSet(\"SPAM_LF\")\n",
    "rules.add_lf_list(LFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Loading data:***\n",
    "### **Load the data: X, X_feats, Y**\n",
    "<p>Note that the utils below is not a part of package but is used to load the necessary data. User have to use some means(which doesn't matter) to load his data(X, X_feats, Y). X is the raw data that is to be passed to LFs, X_feats is feature matrix, of type numpy array and shape (num_instances, num_features) and Y are true labels(if available). Note that we except user to provide feature matrix and feature matrix is not needed in Cage algorithm but it is needed in JL algorithm.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.utils import load_data_to_numpy, get_various_data\n",
    "\n",
    "X, X_feats, Y = load_data_to_numpy()\n",
    "\n",
    "validation_size = 100\n",
    "test_size = 400\n",
    "L_size = 100\n",
    "U_size = 4500\n",
    "n_lfs = len(rules.get_lfs())\n",
    "\n",
    "X_V, Y_V, X_feats_V,_, X_T, Y_T, X_feats_T,_, X_L, Y_L, X_feats_L,_, X_U, X_feats_U,_ = get_various_data(X, Y,\\\n",
    "    X_feats, n_lfs, validation_size, test_size, L_size, U_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Labeling data:***\n",
    "### **Paths**\n",
    "* path_json: path to json file generated by PreLabels\n",
    "* V_path_pkl: path to pkl file generated by PreLabels containing the validation data with true labels\n",
    "* L_path_pkl: path to pkl file generated by PreLabels containing the labeled data with true labels\n",
    "* T_path_pkl: path to pkl file generated by PreLabels containing the test data with true labels\n",
    "* U_path_pkl: path to pkl file generated by PreLabels containing the unlabelled data without true labels\n",
    "* log_path: path to save the log which is generated during the algorithm\n",
    "* params_path: path to save parameters of model\n",
    "\n",
    "<p>Difference between test and labeled data is that labeled data may be used in the algorithm(JL uses it while Cage doesn't) but test data isn't. Make sure that the directory of the files(in above paths) exists. Note that any existing contents in pickle files will be erased.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'data_pipeline/JL/sms_json.json'\n",
    "V_path_pkl = 'data_pipeline/JL/sms_pickle_V.pkl' #validation data - have true labels\n",
    "T_path_pkl = 'data_pipeline/JL/sms_pickle_T.pkl' #test data - have true labels\n",
    "L_path_pkl = 'data_pipeline/JL/sms_pickle_L.pkl' #Labeled data - have true labels\n",
    "U_path_pkl = 'data_pipeline/JL/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "\n",
    "log_path_jl_1 = 'log/JL/sms_log_1.txt' #jl is an algorithm, can be found below\n",
    "params_path = 'params/JL/sms_params.pkl' #file path to store parameters of JL, used below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing PreLabels class and using it to label data**\n",
    "Json file should be generated only once as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.88it/s]\n",
      "100%|██████████| 400/400 [00:29<00:00, 13.69it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.01it/s]\n",
      "100%|██████████| 4500/4500 [05:27<00:00, 13.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import PreLabels\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_V,\n",
    "                               gold_labels=Y_V,\n",
    "                               data_feats=X_feats_V,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(V_path_pkl)\n",
    "sms_noisy_labels.generate_json(path_json) #generating json files once is enough\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_T,\n",
    "                               gold_labels=Y_T,\n",
    "                               data_feats=X_feats_T,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(T_path_pkl)\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_L,\n",
    "                               gold_labels=Y_L,\n",
    "                               data_feats=X_feats_L,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(L_path_pkl)\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_U,\n",
    "                               rules=rules,\n",
    "                               data_feats=X_feats_U,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2) #note that we don't pass gold_labels here, for the unlabelled data\n",
    "sms_noisy_labels.generate_pickle(U_path_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Accessing labeled data:***\n",
    "### **Importing and the use of get_data and get_classes**\n",
    "<p>These functions can be used to extract data from pickle files and json file respectively. Note that these are the files generated using PreLabels.</p>\n",
    "<p>For detailed contents of output, please refer documentation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in data list:  10\n",
      "Shape of feature matrix:  (4500, 1024)\n",
      "Shape of labels matrix:  (4500, 16)\n",
      "Shape of continuous scores matrix :  (4500, 16)\n",
      "Total number of classes:  2\n",
      "Classes dictionary in json file(modified to have integer keys):  {1: 'SPAM', 0: 'HAM'}\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "data_U = get_data(path = U_path_pkl, check_shapes=True)\n",
    "#check_shapes being True(above), asserts for relative shapes of arrays in pickle file\n",
    "print(\"Number of elements in data list: \", len(data_U))\n",
    "print(\"Shape of feature matrix: \", data_U[0].shape)\n",
    "print(\"Shape of labels matrix: \", data_U[1].shape)\n",
    "print(\"Shape of continuous scores matrix : \", data_U[6].shape)\n",
    "print(\"Total number of classes: \", data_U[9])\n",
    "\n",
    "classes = get_classes(path = path_json)\n",
    "print(\"Classes dictionary in json file(modified to have integer keys): \", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Joint Learning(JL) Algorithm:***\n",
    "## **Importing JL class (the algorithm) and declaring an object of it**\n",
    "JL algoritm needs the four types of data:(all this data should be labeled using LFs via PreLabels class)\n",
    "* Unlabeled data(doesn't have true/gold labels)\n",
    "* labeled data(have true/gold labels)\n",
    "* validation data(have true/gold labels)\n",
    "* test data(have true/gold labels)\n",
    "\n",
    "<p>All this data is compulsory for training(passed in fit_and_predict functions). Note that the amount of labeled or validation data can be small, for example they can be of the order of 100 each. Also refer subset selection to find the subset of the data, that is available with you, to label(using a trustable means/SMEs) and use it as 'labeled data' so that the data complements the LFs.</p>\n",
    "<p>The member functions of JL can be choosen to return fm(feature model) or gm(graphical model) predictions. It is highly advised to use the predictions of fm.</p>\n",
    "<p><b>Note:</b> Multiple calls to fit_* functions will train parameters continuously ie, parameters are not reinitialised in fit_* functions. So, to train large data, one can call fit_* functions repeatedly on smaller chunks. Also, in order to perform multiple runs over the algorithm, one need to reinitialise paramters(by creating an object of JL) at the start of each run.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.jl import JL\n",
    "\n",
    "n_features = 1024\n",
    "n_hidden = 512\n",
    "feature_model = 'nn'\n",
    "'''\n",
    "'nn' is neural network. other alternative is 'lr'(logistic regression) which doesn't need n_hidden to be passed\n",
    "during initialisation.\n",
    "''' \n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, feature_model = feature_model, \\\n",
    "        n_hidden = n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict_proba function of JL class with two return values**\n",
    "Here return_gm argument is True which returns predictions/probabilities from graphical model(Cage algorithm) along with feature model. Also note that here test data(path_T) is compulsory and metric_avg is just one value(instead of list as in Cage). The output(probs) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class. \n",
    "<p> Here the order of classes along a row for any instance is the ascending order of values used in enum defined before to hold labels.</p>\n",
    "<p>For more details about arguments, please refer documentation; same should be the case for any of the member functions used from here on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [01:16<03:16,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 28\tbest_epoch: 17\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.4571428571428571\tbest_fm_val_score:0.6470588235294118\n",
      "best_gm_test_score:0.56\tbest_fm_test_score:0.5263157894736842\n",
      "best_gm_test_precision:0.4049586776859504\tbest_fm_test_precision:0.38461538461538464\n",
      "best_gm_test_recall:0.9074074074074074\tbest_fm_test_recall:0.8333333333333334\n",
      "probs_fm shape:  (4500, 2)\n",
      "probs_gm shape:  (4500, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_func_mask = [1,1,1,1,1,1,1] \n",
    "'''\n",
    "One can keep 0s in places where he don't want the specific loss function to be part\n",
    "the final loss function used in training. Refer documentation(spear.JL.core.JL) to understand\n",
    "the which index of loss_func_mask refers to what loss function.\n",
    "\n",
    "Note: the loss_func_mask above may not be the optimal mask for sms dataset. We have to try\n",
    "      some other masks too, to find the best one that gives good accuracies.\n",
    "'''\n",
    "batch_size = 150\n",
    "lr_fm = 0.0005\n",
    "lr_gm = 0.01\n",
    "use_accuracy_score = False\n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, feature_model = feature_model, \\\n",
    "        n_hidden = n_hidden)\n",
    "\n",
    "probs_fm, probs_gm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)\n",
    "print(\"probs_gm shape: \", probs_gm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict_proba function of JL class with one return value**\n",
    "Here return_gm argument is False which returns predictions only from feature model. Also note the feature_model used here is 'lr'(logistic regression). The output(probs) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class.\n",
    "<p> Here the order of classes along a row for any instance is the ascending order of values used in enum defined before to hold labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:34<02:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 22\tbest_epoch: 14\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.4571428571428571\tbest_fm_val_score:0.8148148148148148\n",
      "best_gm_test_score:0.5798816568047337\tbest_fm_test_score:0.9174311926605504\n",
      "best_gm_test_precision:0.4260869565217391\tbest_fm_test_precision:0.9090909090909091\n",
      "best_gm_test_recall:0.9074074074074074\tbest_fm_test_recall:0.9259259259259259\n",
      "probs_fm shape:  (4500, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feature_model = 'lr' #in this case, n_hidden need not be passed as an argument\n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, feature_model = feature_model)\n",
    "\n",
    "probs_fm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = False, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of JL class**\n",
    "The return_gm argument is set to True. The output(probs) is a numpy matrix of shape (num_instances,) containing integers(because need_strings is False), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [01:03<03:01,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 26\tbest_epoch: 15\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.4571428571428571\tbest_fm_val_score:0.7857142857142857\n",
      "best_gm_test_score:0.5764705882352941\tbest_fm_test_score:0.7868852459016393\n",
      "best_gm_test_precision:0.4224137931034483\tbest_fm_test_precision:0.7058823529411765\n",
      "best_gm_test_recall:0.9074074074074074\tbest_fm_test_recall:0.8888888888888888\n",
      "labels_fm shape:  (4500,)\n",
      "labels_gm shape:  (4500,)\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feature_model = 'nn' #resetting feature_model as 'nn'(neural network) for further trainings\n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, feature_model = feature_model, \\\n",
    "        n_hidden = n_hidden)\n",
    "\n",
    "labels_fm, labels_gm = jl.fit_and_predict(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary', \\\n",
    "    need_strings = False)\n",
    "\n",
    "print(\"labels_fm shape: \", labels_fm.shape)\n",
    "print(\"labels_gm shape: \", labels_gm.shape)\n",
    "print(type(labels_fm[0]))\n",
    "print(type(labels_gm[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of JL class**\n",
    "The return_gm argument is set to True. The output(probs) is a numpy matrix of shape (num_instances,) containing strings(because need_strings is True), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [01:23<03:06,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 31\tbest_epoch: 20\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.4571428571428571\tbest_fm_val_score:0.7333333333333334\n",
      "best_gm_test_score:0.5568181818181819\tbest_fm_test_score:0.7692307692307692\n",
      "best_gm_test_precision:0.4016393442622951\tbest_fm_test_precision:0.6578947368421053\n",
      "best_gm_test_recall:0.9074074074074074\tbest_fm_test_recall:0.9259259259259259\n",
      "labels_fm shape:  (4500,)\n",
      "labels_gm shape:  (4500,)\n",
      "<class 'numpy.str_'>\n",
      "<class 'numpy.str_'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, feature_model = feature_model, \\\n",
    "        n_hidden = n_hidden)\n",
    "\n",
    "labels_fm, labels_gm = jl.fit_and_predict(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary', \\\n",
    "    need_strings = True)\n",
    "\n",
    "print(\"labels_fm shape: \", labels_fm.shape)\n",
    "print(\"labels_gm shape: \", labels_gm.shape)\n",
    "print(type(labels_fm[0]))\n",
    "print(type(labels_gm[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save parameters**\n",
    "<p> Make sure that the directory of save_path file exists. Note that any existing contents in pickle file will be erased.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl.save_params(save_path = params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl_2 = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, feature_model = feature_model, \\\n",
    "        n_hidden = n_hidden)\n",
    "jl_2.load_params(load_path = params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict_fm/gm_proba functions of JL class**\n",
    "The output(probs_test) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class.\n",
    "<p> Here the order of classes along a row for any instance is the ascending order of values used in enum defined before to hold labels.</p>\n",
    "<p>Note that predict_fm_proba takes feature matrix(can also be obtained from pickle file using get_data()) as argument while predict_gm_proba takes pickle file(containing labels given by LFs) as argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in JL class. Hope you have loaded parameters.\n",
      "Warning: Predict is used before training any paramters in JL class. Hope you have loaded parameters.\n",
      "probs_fm_test shape:  (400, 2)\n",
      "probs_gm_test shape:  (400, 2)\n"
     ]
    }
   ],
   "source": [
    "probs_fm_test = jl_2.predict_fm_proba(x_test = X_feats_T)\n",
    "probs_gm_test = jl_2.predict_gm_proba(path_test = T_path_pkl, qc = 0.85)\n",
    "\n",
    "print(\"probs_fm_test shape: \", probs_fm_test.shape)\n",
    "print(\"probs_gm_test shape: \", probs_gm_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict_fm/gm functions of JL class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers(strings) if need_strings is Flase(True), having the classes of each instance. Just the use case with need_strings as False is displayed here. \n",
    "<p>Note that predict_fm takes feature matrix(can also be obtained from pickle file using get_data()) as argument while predict_gm takes pickle file(containing labels given by LFs) as argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in JL class. Hope you have loaded parameters.\n",
      "Warning: Predict is used before training any paramters in JL class. Hope you have loaded parameters.\n",
      "labels_fm_test shape:  (400,)\n",
      "labels_gm_test shape:  (400,)\n",
      "accuracy_score of gm:  0.805 | fm:  0.925\n",
      "f1_score of gm:  0.5568181818181819 | fm:  0.7692307692307692\n"
     ]
    }
   ],
   "source": [
    "labels_fm_test = jl_2.predict_fm(x_test = X_feats_T, need_strings=False)\n",
    "labels_gm_test = jl_2.predict_gm(path_test = T_path_pkl, qc = 0.85, need_strings=False)\n",
    "\n",
    "print(\"labels_fm_test shape: \", labels_fm_test.shape)\n",
    "print(\"labels_gm_test shape: \", labels_gm_test.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#Y_T is true labels of test data, type is numpy array of shape (num_instances,)\n",
    "print(\"accuracy_score of gm: \", accuracy_score(Y_T, labels_gm_test), \"| fm: \", accuracy_score(Y_T, labels_fm_test))\n",
    "print(\"f1_score of gm: \", f1_score(Y_T, labels_gm_test, average = 'binary'), \"| fm: \", f1_score(Y_T, labels_fm_test, average = 'binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting numpy array of integers to enums**\n",
    "The below utility from spear can help convert return values of predict_fm, predict_gm(obtained when need_strings is Flase) to a numpy array of enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enum 'ClassLabels'>\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_enum\n",
    "\n",
    "probs_fm_test_enum = get_enum(np_array = labels_fm_test, enm = ClassLabels) \n",
    "#the second argument is the Enum class defined at beginning\n",
    "print(type(probs_fm_test_enum[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
