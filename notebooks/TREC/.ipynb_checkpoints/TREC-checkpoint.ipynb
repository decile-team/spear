{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import enum\n",
    "import numpy as np\n",
    "\n",
    "from spear.labeling import labeling_function, LFSet, ABSTAIN, preprocessor\n",
    "from examples.TREC.preprocessor import convert_to_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"DESC\": \"DESCRIPTION\",\n",
    "            \"ENTY\": \"ENTITY\",\n",
    "            \"HUM\": \"HUMAN\",\n",
    "            \"ABBR\": \"ABBREVIATION\",\n",
    "            \"LOC\": \"LOCATION\",\n",
    "            \"NUM\": \"NUMERIC\"}\n",
    "\n",
    "class ClassLabels(enum.Enum):\n",
    "    DESCRIPTION     = 0\n",
    "    ENTITY          = 1\n",
    "    HUMAN           = 2\n",
    "    ABBREVIATION    = 3\n",
    "    LOCATION        = 4\n",
    "    NUMERIC         = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules(file_name='rules.txt'):\n",
    "    rules = LFSet(\"TREC_LFS\")\n",
    "    \n",
    "    with open(file_name, 'r', encoding='latin1') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            list_in = line.strip().split(\"\\t\")\n",
    "            label = ClassLabels[label_map[list_in[0]]]\n",
    "            pattern = list_in[1]\n",
    "            rule_name = \"rule\"+str(i)\n",
    "            \n",
    "            @labeling_function(name=rule_name,resources=dict(pattern=pattern,output=label),pre=[convert_to_lower],label=label)\n",
    "            def f(x,**kwargs):\n",
    "                result = re.findall(kwargs[\"pattern\"], x)\n",
    "                if result:\n",
    "                    return kwargs[\"output\"]\n",
    "                else:\n",
    "                    return ABSTAIN\n",
    "\n",
    "            rules.add_lf(f)\n",
    "            i = i+1\n",
    "    return rules\n",
    "\n",
    "rules = load_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DICT = {\"DESCRIPTION\": 0, \"ENTITY\": 1, \"HUMAN\": 2, \"ABBREVIATION\": 3, \"LOCATION\": 4, \"NUMERIC\": 5}\n",
    "\n",
    "def load_data(mode):\n",
    "    label_map = {\"DESC\": 0, \"ENTY\": 1, \"HUM\": 2, \"ABBR\": 3, \"LOC\": 4,\"NUM\": 5}\n",
    "    data = []\n",
    "\n",
    "    with open(mode + '.txt', 'r', encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            label = LABEL_DICT[label_map[line.split()[0].split(\":\")[0]]]\n",
    "            if mode == \"test\":\n",
    "                sentence = (\" \".join(line.split()[1:]))\n",
    "            else:\n",
    "                sentence = (\" \".join(line.split(\":\")[1:])).lower().strip()\n",
    "            data.append((sentence, label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import preprocessor\n",
    "\n",
    "@preprocessor()\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import PreLabels\n",
    "from notebooks.TREC.utils import load_data_to_numpy\n",
    "\n",
    "X_V, X_feats_V, Y_V = load_data_to_numpy(file_name='valid.txt')\n",
    "X_T, X_feats_T, Y_T = load_data_to_numpy(file_name='test.txt')\n",
    "X, X_feats, Y = load_data_to_numpy(file_name='train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L, X_feats_L, Y_L, X_U, X_feats_U, Y_U = X[:100] , X_feats[:100], Y[:100], X[100:], X_feats[100:], Y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (5352,), (500,), (500,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_L.shape, X_U.shape, X_V.shape, X_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, X_feats, Y = load_data_to_numpy()\n",
    "# Y = np.array([ClassLabels[x].value for x in Y])\n",
    "\n",
    "# trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "#                                data=X,\n",
    "#                                gold_labels=Y,\n",
    "#                                data_feats=X_feats,\n",
    "#                                rules=rules,\n",
    "#                                labels_enum=ClassLabels,\n",
    "#                                num_classes=6)\n",
    "# L,S = trec_noisy_labels.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper.utils import load_data_to_numpy, get_various_data\n",
    "\n",
    "# X_V = \n",
    "\n",
    "# X, X_feats, Y = load_data_to_numpy()\n",
    "\n",
    "# validation_size = 152\n",
    "# test_size = 500\n",
    "# L_size = 100\n",
    "# U_size = 4700\n",
    "# n_lfs = len(rules.get_lfs())\n",
    "\n",
    "# X_V, Y_V, X_feats_V,_, X_T, Y_T, X_feats_T,_, X_L, Y_L, X_feats_L,_, X_U, X_feats_U,_ = get_various_data(X, Y,\\\n",
    "#     X_feats, n_lfs, validation_size, test_size, L_size, U_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'data_pipeline/trec_json.json'\n",
    "V_path_pkl = 'data_pipeline/trec_pickle_V.pkl' #validation data - have true labels\n",
    "T_path_pkl = 'data_pipeline/trec_pickle_T.pkl' #test data - have true labels\n",
    "L_path_pkl = 'data_pipeline/trec_pickle_L.pkl' #Labeled data - have true labels\n",
    "U_path_pkl = 'data_pipeline/trec_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "\n",
    "log_path_cage_1 = 'log/trec_cage_log_1.txt' #cage is an algorithm, can be found below\n",
    "log_path_jl_1 = 'log/trec_jl_log_1.txt' #jl is an algorithm, can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 2826.02it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 4532.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2971.02it/s]\n",
      "100%|██████████| 5352/5352 [00:01<00:00, 4132.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import PreLabels\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_V,\n",
    "                               gold_labels=Y_V,\n",
    "                               data_feats=X_feats_V,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(V_path_pkl)\n",
    "trec_noisy_labels.generate_json(path_json) #generating json files once is enough\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_T,\n",
    "                               gold_labels=Y_T,\n",
    "                               data_feats=X_feats_T,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(T_path_pkl)\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_L,\n",
    "                               gold_labels=Y_L,\n",
    "                               data_feats=X_feats_L,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(L_path_pkl)\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_U,\n",
    "                               rules=rules,\n",
    "                               data_feats=X_feats_U,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(U_path_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in data list:  10\n",
      "Shape of feature matrix:  (5352, 1024)\n",
      "Shape of labels matrix:  (5352, 68)\n",
      "Shape of continuous scores matrix :  (5352, 68)\n",
      "Total number of classes:  6\n",
      "Classes dictionary in json file(modified to have integer keys):  {0: 'DESCRIPTION', 1: 'ENTITY', 2: 'HUMAN', 3: 'ABBREVIATION', 4: 'LOCATION', 5: 'NUMERIC'}\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "data_U = get_data(path = U_path_pkl, check_shapes=True)\n",
    "#check_shapes being True(above), asserts for relative shapes of arrays in pickle file\n",
    "print(\"Number of elements in data list: \", len(data_U))\n",
    "print(\"Shape of feature matrix: \", data_U[0].shape)\n",
    "print(\"Shape of labels matrix: \", data_U[1].shape)\n",
    "print(\"Shape of continuous scores matrix : \", data_U[6].shape)\n",
    "print(\"Total number of classes: \", data_U[9])\n",
    "\n",
    "classes = get_classes(path = path_json)\n",
    "print(\"Classes dictionary in json file(modified to have integer keys): \", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from spear.JL import JL\n",
    "\n",
    "# n_features = 1024\n",
    "# n_hidden = 512\n",
    "# feature_model = 'lr'\n",
    "\n",
    "# jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "#         feature_model = feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'log/trec_jl_log_1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-408965790f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpath_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT_path_pkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_fm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_fm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_gm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlr_gm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_accuracy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_accuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_path_jl_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'macro')\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_fm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/decile/spear/spear/jl/core.py\u001b[0m in \u001b[0;36mfit_and_predict_proba\u001b[0;34m(self, path_L, path_U, path_V, path_T, loss_func_mask, batch_size, lr_fm, lr_gm, use_accuracy_score, path_log, return_gm, n_epochs, start_len, stop_len, is_qt, is_qc, qt, qc, metric_avg)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpath_log\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \t\t\tfile.write(\"JL log:\\tn_classes: {}\\tn_LFs: {}\\tn_features: {}\\tn_hidden: {}\\tfeature_model:{}\\tlr_fm: {}\\tlr_gm:{}\\tuse_accuracy_score: {}\\tn_epochs:{}\\tstart_len: {}\\tstop_len:{}\\n\".format(\\\n\u001b[1;32m    320\u001b[0m \t\t\t\tself.n_classes, self.n_lfs, self.n_features, self.n_hidden, self.feature_based_model, lr_fm, lr_gm, use_accuracy_score, n_epochs, start_len, stop_len))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'log/trec_jl_log_1.txt'"
     ]
    }
   ],
   "source": [
    "from spear.jl import JL\n",
    "\n",
    "loss_func_mask = [1,1,1,1,1,1,1]\n",
    "'''\n",
    "One can keep 0s in places where he don't want the specific loss function to be part\n",
    "the final loss function used in training. Refer documentation(spear.JL.core.JL) to understand\n",
    "the which index of loss_func_mask refers to what loss function.\n",
    "Note: the loss_func_mask may not be the optimal mask for sms dataset.\n",
    "'''\n",
    "batch_size = 150\n",
    "lr_fm = 0.0005\n",
    "lr_gm = 0.01\n",
    "use_accuracy_score = False\n",
    "feature_model = 'nn'\n",
    "n_features = 1024\n",
    "n_hidden = 512\n",
    "n_lfs = len(rules.get_lfs())\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)\n",
    "\n",
    "probs_fm, probs_gm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'macro')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)\n",
    "print(\"probs_gm shape: \", probs_gm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
